{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Build-Vocabulary\" data-toc-modified-id=\"Build-Vocabulary-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Build Vocabulary</a></span></li><li><span><a href=\"#Unigram-spell-checker\" data-toc-modified-id=\"Unigram-spell-checker-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Unigram spell checker</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluate-accuracy\" data-toc-modified-id=\"Evaluate-accuracy-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Evaluate accuracy</a></span></li></ul></li><li><span><a href=\"#Bigram-spell-checker\" data-toc-modified-id=\"Bigram-spell-checker-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Bigram spell checker</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluating\" data-toc-modified-id=\"Evaluating-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Evaluating</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:29.633661Z",
     "start_time": "2021-04-16T10:05:29.614943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:29.882418Z",
     "start_time": "2021-04-16T10:05:29.862594Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import os,sys,inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir) \n",
    "\n",
    "import data\n",
    "from data import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:30.453563Z",
     "start_time": "2021-04-16T10:05:30.312618Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = utils.get_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:30.545217Z",
     "start_time": "2021-04-16T10:05:30.525864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': ['they', 'can', 'go', 'quite', 'farst'],\n",
       " 'corrected': ['they', 'can', 'go', 'quite', 'fast'],\n",
       " 'indexes': [4]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:31.097177Z",
     "start_time": "2021-04-16T10:05:31.077236Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr = [x['original'] for x in train]\n",
    "y_tr = [x['corrected'] for x in train]\n",
    "y_tr_idx =  [x['indexes'] for x in train]\n",
    "\n",
    "X_te = [x['original'] for x in test]\n",
    "y_te = [x['corrected'] for x in test]\n",
    "y_te_idx =  [x['indexes'] for x in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:31.360312Z",
     "start_time": "2021-04-16T10:05:31.305245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocabulary)=|3451\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "\n",
    "bigram_finder = BigramCollocationFinder.from_documents(X_tr)\n",
    "bigram_freq_dict = dict(bigram_finder.ngram_fd.items())\n",
    "list(bigram_freq_dict.keys())[0:10]\n",
    "\n",
    "import itertools\n",
    "vocabulary = set(list(itertools.chain(*bigram_freq_dict.keys())))\n",
    "print(f'len(vocabulary)=|{len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:32.405196Z",
     "start_time": "2021-04-16T10:05:32.382650Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "corrected_sentences = [train[n]['corrected'] for n in range(len(train))]\n",
    "corrected_words = [word.lower() for sentence in corrected_sentences for word in sentence]\n",
    "unique_corrected_words = set(corrected_words)\n",
    "n_total_words = len(corrected_words)\n",
    "vocabulary = unique_corrected_words\n",
    "\n",
    "def build_unigrams(corrected_words):\n",
    "    return Counter(corrected_words) \n",
    "\n",
    "def prob(word, unigrams, n_total_words):\n",
    "    word = word.lower()\n",
    "    word_counts = unigrams[word]\n",
    "    return word_counts / n_total_words\n",
    "\n",
    "# Test your code with the following\n",
    "# assert(unigram(\"me\")==87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:33.245643Z",
     "start_time": "2021-04-16T10:05:33.226078Z"
    }
   },
   "outputs": [],
   "source": [
    "unigrams = build_unigrams(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:33.450732Z",
     "start_time": "2021-04-16T10:05:33.431333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002200926223118896\n",
      "0.003989178779402999\n",
      "0.00018341051859324133\n"
     ]
    }
   ],
   "source": [
    "print(prob('house', unigrams, n_total_words))\n",
    "print(prob('me', unigrams, n_total_words))\n",
    "print(prob('television', unigrams, n_total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:33.657966Z",
     "start_time": "2021-04-16T10:05:33.638333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 87, 4)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams['house'], unigrams['me'], unigrams['television']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:34.482362Z",
     "start_time": "2021-04-16T10:05:34.463937Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:34.733054Z",
     "start_time": "2021-04-16T10:05:34.713106Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_candidates(token, vocabulary):\n",
    "    # Write your code here.\n",
    "    distance_token_to_words = {word:edit_distance(word,token.lower()) for word in vocabulary}\n",
    "    minimum_distance = min(distance_token_to_words.values())\n",
    "    return sorted([word for word, distance in distance_token_to_words.items() \n",
    "                   if distance == minimum_distance], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:35.041293Z",
     "start_time": "2021-04-16T10:05:34.972716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test your code as follows\n",
    "assert get_candidates(\"minde\",vocabulary) == ['mine', 'mind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:37.885103Z",
     "start_time": "2021-04-16T10:05:35.197380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.9 ms ± 760 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit get_candidates(\"min\",vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram spell checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:07:35.355198Z",
     "start_time": "2021-04-16T10:07:35.336052Z"
    }
   },
   "outputs": [],
   "source": [
    "def correct_tokens(tokenized_sentence, unigrams, n_total_words):\n",
    "    tokenized_sentence = tokenized_sentence.copy()\n",
    "    \n",
    "    for index,word in enumerate(tokenized_sentence):\n",
    "        if (word and word.lower()) not in unique_corrected_words:\n",
    "            candidates = {candidate:prob(candidate, unigrams, n_total_words) for candidate in get_candidates(word,vocabulary)}\n",
    "            best_candidate  = max(candidates, key=candidates.get)\n",
    "            tokenized_sentence[index] = best_candidate\n",
    "\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:07:35.641040Z",
     "start_time": "2021-04-16T10:07:35.571938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'white', 'cat']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_tokens(tokenized_sentence = [\"this\", \"whitr\", \"cat\"],  \n",
    "               unigrams=unigrams, \n",
    "               n_total_words=n_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:07:35.818340Z",
     "start_time": "2021-04-16T10:07:35.757118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'my', 'cat']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_tokens(tokenized_sentence = [\"this\",\"is\",\"my\",\"caat\"],  \n",
    "               unigrams=unigrams, \n",
    "               n_total_words=n_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:07:36.006028Z",
     "start_time": "2021-04-16T10:07:35.936746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Joan', 'likes', 'pigs', 'and', 'cats']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_tokens(tokenized_sentence = [\"Joan\",\"likes\",\"pissa\",\"and\",\"cats\"],  \n",
    "               unigrams=unigrams, \n",
    "               n_total_words=n_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:07:36.133582Z",
     "start_time": "2021-04-16T10:07:36.112791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'pizza' in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:07:46.165989Z",
     "start_time": "2021-04-16T10:07:46.147178Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(test,  unigrams, n_total_words):\n",
    "    # Write your code here\n",
    "    count_total_words = 0\n",
    "    count_corrected_words = 0\n",
    "    for sentence in test:\n",
    "        corrected_sentence = correct_tokens(sentence['original'], unigrams, n_total_words)  \n",
    "        count_total_words +=len(sentence['corrected'])\n",
    "        count_corrected_words += sum(corrected_sentence[n] == sentence['corrected'][n] \n",
    "                                     for n in range(len(sentence['corrected'])))\n",
    "    return count_corrected_words/count_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:07:56.201531Z",
     "start_time": "2021-04-16T10:07:46.563698Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8380281690140845"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test, unigrams, n_total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram spell checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:08:16.461754Z",
     "start_time": "2021-04-16T10:08:16.410003Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from collections import Counter\n",
    "\n",
    "corrected_sentences = [train[n]['corrected'] for n in range(len(train))]\n",
    "corrected_words = [word.lower() for sentence in corrected_sentences for word in sentence]\n",
    "unique_corrected_words = set(corrected_words)\n",
    "finder = BigramCollocationFinder.from_words(corrected_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:08:17.160524Z",
     "start_time": "2021-04-16T10:08:17.141192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they', 'can', 'go', 'quite', 'fast', 'this', 'was', 'a', 'royal', 'enfield']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:08:20.807410Z",
     "start_time": "2021-04-16T10:08:20.786467Z"
    }
   },
   "outputs": [],
   "source": [
    "unigram_freq_dict = build_unigrams(corrected_words)\n",
    "bigram_freq_dict  = dict(finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:08:23.797604Z",
     "start_time": "2021-04-16T10:08:23.778754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they', 'can', 'go', 'quite', 'fast', 'this', 'was', 'a', 'royal', 'enfield']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:32.875611Z",
     "start_time": "2021-04-16T10:15:32.851040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('they', 'can'), 4),\n",
       " (('can', 'go'), 5),\n",
       " (('go', 'quite'), 1),\n",
       " (('quite', 'fast'), 1),\n",
       " (('fast', 'this'), 1),\n",
       " (('this', 'was'), 4),\n",
       " (('was', 'a'), 35),\n",
       " (('a', 'royal'), 3),\n",
       " (('royal', 'enfield'), 5),\n",
       " (('enfield', '_'), 1)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools \n",
    "def top_k_dict(d, top_k = 10):\n",
    "    return [(x,bigram_freq_dict[x]) for k,x in enumerate(bigram_freq_dict) if k<top_k]\n",
    "\n",
    "top_k_dict(bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can notice that actually the bigrams where not computed correctly because we concatenated all the data from the  corrected sentences into a single list `corrected_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:36.910743Z",
     "start_time": "2021-04-16T10:15:36.890650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': ['they', 'can', 'go', 'quite', 'farst'],\n",
       " 'corrected': ['they', 'can', 'go', 'quite', 'fast'],\n",
       " 'indexes': [4]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:37.675498Z",
     "start_time": "2021-04-16T10:15:37.654348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': ['this', 'was', 'a', 'Royl', 'Enfield', 'Consulatoin', '?', '_'],\n",
       " 'corrected': ['this', 'was', 'a', 'Royal', 'Enfield', '_', '?', '_'],\n",
       " 'indexes': [3, 5]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how the bigram `(fast,this)` appears in the `bigram_freq_dict` but in reality this might not even be in the training data.\n",
    "\n",
    "It is a consequence of concatenating the training dat into a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:48.556519Z",
     "start_time": "2021-04-16T10:15:48.534120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('they', 'can'),\n",
       " ('can', 'go'),\n",
       " ('go', 'quite'),\n",
       " ('quite', 'fast'),\n",
       " ('fast', 'this'),\n",
       " ('this', 'was')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigram_freq_dict)[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could build the bigrams avoiding this issue using `BigramCollocationFinder.from_documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:49.547195Z",
     "start_time": "2021-04-16T10:15:49.524151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(('they', 'can'), 1), (('can', 'go'), 1), (('go', 'quite'), 1), (('quite', 'fast'), 1), (('this', 'was'), 1), (('was', 'a'), 1), (('a', 'Royal'), 1), (('Royal', 'Enfield'), 1), (('Enfield', '_'), 1), (('_', '?'), 1), (('?', '_'), 1)])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_finder = BigramCollocationFinder.from_documents([['they', 'can', 'go', 'quite', 'fast'],\n",
    "                                                         ['this', 'was', 'a', 'Royal', 'Enfield', '_', '?', '_']])\n",
    "bigram_freq_dict_aux = bigram_finder.ngram_fd.items()\n",
    "bigram_freq_dict_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build again `bigram_freq_dict_aux` using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:51.950586Z",
     "start_time": "2021-04-16T10:15:51.897884Z"
    }
   },
   "outputs": [],
   "source": [
    "corrected_sentences = [train[n]['corrected'] for n in range(len(train))]\n",
    "finder = BigramCollocationFinder.from_documents(corrected_sentences)\n",
    "bigram_freq_dict = dict(finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:52.292290Z",
     "start_time": "2021-04-16T10:15:52.271204Z"
    }
   },
   "outputs": [],
   "source": [
    "def prob_word(word, word_to_count, n_total_words, n_vocabulary):\n",
    "    # Write your code here.\n",
    "    word = word.lower()\n",
    "    word_counts = word_to_count[word]\n",
    "    return word_counts / n_total_words\n",
    "\n",
    "#def prob_word(word, word_to_count, n_total_words, n_vocabulary): \n",
    "#    return (word_to_count[word]+1) / (n_total_words+ n_vocabulary)\n",
    "\n",
    "def bigrams_starting_by(word, bigram_freq_dictionary): \n",
    "    return [t for t in list(bigram_freq_dict.keys()) if t[0] == word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:53.132777Z",
     "start_time": "2021-04-16T10:15:53.112777Z"
    }
   },
   "outputs": [],
   "source": [
    "n_total_words, len(unique_corrected_words)\n",
    "n_vocabulary = len(unique_corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:53.427802Z",
     "start_time": "2021-04-16T10:15:53.405287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 'see'),\n",
       " ('dog', 'and'),\n",
       " ('dog', 'run'),\n",
       " ('dog', 'is'),\n",
       " ('dog', ','),\n",
       " ('dog', 'had'),\n",
       " ('dog', 'for'),\n",
       " ('dog', 'Toby'),\n",
       " ('dog', '.'),\n",
       " ('dog', 'it')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_starting_by('dog', bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:54.448875Z",
     "start_time": "2021-04-16T10:15:54.428532Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_dictionary_value(bigram, bigram_freq_dict):\n",
    "    try:\n",
    "        return bigram_freq_dict[bigram]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def count_bigrams(list_bigrams, bigram_freq_dict): \n",
    "    return sum([return_dictionary_value(bigram, bigram_freq_dict) for bigram in list_bigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:55.315258Z",
     "start_time": "2021-04-16T10:15:55.295249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_bigrams([('dog','is')], bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:56.485423Z",
     "start_time": "2021-04-16T10:15:56.465691Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def probability_bigram(word1, word2, bigram_freq_dict):\n",
    "    if count_bigrams([(word1,word2)], bigram_freq_dict) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return count_bigrams([(word1,word2)], bigram_freq_dict)/count_bigrams(bigrams_starting_by(word1,bigram_freq_dict), bigram_freq_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:56.905151Z",
     "start_time": "2021-04-16T10:15:56.883264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090909090909091"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('dog','is')\n",
    "probability_bigram(word1,word2, bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:57.814659Z",
     "start_time": "2021-04-16T10:15:57.792963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1263537906137184"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('was','a')\n",
    "probability_bigram(word1,word2, bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:58.031315Z",
     "start_time": "2021-04-16T10:15:58.008470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('was','cat')\n",
    "probability_bigram(word1, word2, bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:15:59.681048Z",
     "start_time": "2021-04-16T10:15:59.660510Z"
    }
   },
   "outputs": [],
   "source": [
    "def interpolation_probability(word1, word2, bigram_freq_dict, n_vocabulary, lambda_1 = 0.3): \n",
    "    return (1-lambda_1)*probability_bigram(word1, word2, bigram_freq_dict) +\\\n",
    "            lambda_1*prob_word(word2, unigrams, n_total_words, n_vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:16:00.268456Z",
     "start_time": "2021-04-16T10:16:00.245799Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09686619623303266"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('was','a')\n",
    "interpolation_probability(word1, word2, bigram_freq_dict, n_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:16:01.167029Z",
     "start_time": "2021-04-16T10:16:01.145225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00020633683341739648"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('was','cat')\n",
    "interpolation_probability(word1, word2, bigram_freq_dict, n_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:16:01.439032Z",
     "start_time": "2021-04-16T10:16:01.417347Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_candidates(token, vocabulary, max_dist):\n",
    "    distance_token_to_words = {word:edit_distance(word,token.lower()) for word in vocabulary}\n",
    "    minimum_distance = min(distance_token_to_words.values())\n",
    "    if minimum_distance < max_dist:\n",
    "        return sorted([word for word, distance in distance_token_to_words.items() if distance == minimum_distance])\n",
    "    return [token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:16:02.424898Z",
     "start_time": "2021-04-16T10:16:02.361155Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house', 'huge', 'use']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates('huse', vocabulary, max_dist=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:16:02.648872Z",
     "start_time": "2021-04-16T10:16:02.575458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ditch', 'witch']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates('kitch', vocabulary, max_dist=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:16:02.821226Z",
     "start_time": "2021-04-16T10:16:02.801413Z"
    }
   },
   "outputs": [],
   "source": [
    "#%timeit get_candidates('hose', vocabulary, max_dist=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:16:03.675866Z",
     "start_time": "2021-04-16T10:16:03.654352Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def correct_with_bigrams(sentence, vocabulary):\n",
    "    for index,word in enumerate(sentence):\n",
    "        if ((word and word.lower()) not in vocabulary) and (not word[0].isupper()):\n",
    "            if index == 0: \n",
    "                previous_word = '.'\n",
    "            else:\n",
    "                previous_word = sentence[index-1].lower()\n",
    "            candidates = {candidate:interpolation_probability(previous_word, candidate, bigram_freq_dict,\n",
    "                                                              n_vocabulary, lambda_1=0.3) for candidate in get_candidates(word,vocabulary,max_dist=2)}\n",
    "            \n",
    "            \n",
    "            sentence[index] = max(candidates, key=candidates.get)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:16:03.990108Z",
     "start_time": "2021-04-16T10:16:03.925139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'big', 'house']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_with_bigrams([\"the\",\"big\",\"hose\"], vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:16:04.237812Z",
     "start_time": "2021-04-16T10:16:04.161431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hole': 0.0019202404016783775,\n",
       " 'home': 0.0006602778669356688,\n",
       " 'hope': 0.00012380210005043788,\n",
       " 'horse': 1.37557888944931e-05,\n",
       " 'hoses': 1.37557888944931e-05,\n",
       " 'house': 0.00660255290938049,\n",
       " 'lose': 4.12673666834793e-05,\n",
       " 'nose': 4.12673666834793e-05,\n",
       " 'rose': 6.87789444724655e-05,\n",
       " 'those': 2.75115777889862e-05,\n",
       " 'whose': 1.37557888944931e-05}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'hose'\n",
    "previous_word = 'the'\n",
    "candidates = {candidate:interpolation_probability(previous_word, candidate, bigram_freq_dict,n_vocabulary, lambda_1=0.3) for candidate in get_candidates(word,vocabulary,max_dist=2)}\n",
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-16T10:16:08.243Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_bigrams(test, vocabulary):\n",
    "    # Write your code here\n",
    "    count_total_words = 0\n",
    "    count_corrected_words = 0\n",
    "    mistakes = []\n",
    "    for m,sentence in enumerate(test):\n",
    "        s_true = sentence['corrected']\n",
    "        s_hat  = correct_with_bigrams(sentence['original'].copy(), vocabulary)\n",
    "        count_total_words  += len(s_true)\n",
    "        correct_predictions = sum(s_hat[n] == s_true[n] for n in range(len(s_true)))\n",
    "        count_corrected_words += correct_predictions\n",
    "        if correct_predictions != len(s_true):\n",
    "            mistakes.append(m)\n",
    "            \n",
    "    return count_corrected_words/count_total_words, mistakes\n",
    "\n",
    "acc, mistakes = accuracy_bigrams(test, vocabulary)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-16T10:16:09.840Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 14\n",
    "sentence = test[mistakes[i]]\n",
    "x = sentence[\"original\"]\n",
    "y_true = sentence[\"corrected\"]\n",
    "y_hat  = correct_with_bigrams(sentence['original'].copy(), vocabulary)\n",
    "\n",
    "print(f'mistake indices = {sentence[\"indexes\"]}')\n",
    "print(f'x      = {x}')\n",
    "print(f'y_hat  = {y_hat}')\n",
    "print(f'y_true = {y_true}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-16T10:16:10.829Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 4\n",
    "sentence = test[mistakes[i]]\n",
    "\n",
    "x      = sentence[\"original\"]\n",
    "y_true = sentence[\"corrected\"]\n",
    "y_hat  = correct_with_bigrams(sentence['original'].copy(), vocabulary)\n",
    "\n",
    "print(f'mistakes[i]={mistakes[i]}')\n",
    "print(f'mistake indices = {sentence[\"indexes\"]}')\n",
    "print(f'x      = {x}')\n",
    "print(f'y_hat  = {y_hat}')\n",
    "print(f'y_true = {y_true}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T09:44:05.852052Z",
     "start_time": "2021-04-16T09:44:05.849102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': ['on', 'sundays', 'I', 'go', 'to', 'church', '.'],\n",
       " 'corrected': ['on', 'sundays', 'I', 'go', 'to', 'church', '.'],\n",
       " 'indexes': []}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T09:44:06.740212Z",
     "start_time": "2021-04-16T09:44:06.637516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistakes[i]=33\n",
      "mistake indices = [15, 26]\n",
      "x      = ['The', 'murder', 'man', 'has', 'a', 'black', 'beard', 'The', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killd', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'Hill', 'the', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n",
      "y_hat  = ['The', 'murder', 'man', 'has', 'a', 'black', 'heard', 'The', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killed', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'Hill', 'the', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n",
      "y_true = ['The', 'murder', 'man', 'has', 'a', 'black', 'beard', 'The', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killed', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'Hill', 'there', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n"
     ]
    }
   ],
   "source": [
    "i = 20\n",
    "sentence = test[mistakes[i]]\n",
    "x      = sentence[\"original\"]\n",
    "y_true = sentence[\"corrected\"]\n",
    "y_hat  = correct_with_bigrams(sentence['original'].copy(), vocabulary)\n",
    "\n",
    "\n",
    "print(f'mistakes[i]={mistakes[i]}')\n",
    "print(f'mistake indices = {sentence[\"indexes\"]}')\n",
    "print(f'x      = {x}')\n",
    "print(f'y_hat  = {y_hat}')\n",
    "print(f'y_true = {y_true}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
