{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Build-Vocabulary\" data-toc-modified-id=\"Build-Vocabulary-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Build Vocabulary</a></span></li><li><span><a href=\"#Unigram-spell-checker\" data-toc-modified-id=\"Unigram-spell-checker-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Unigram spell checker</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluate-accuracy\" data-toc-modified-id=\"Evaluate-accuracy-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Evaluate accuracy</a></span></li></ul></li><li><span><a href=\"#Bigram-spell-checker\" data-toc-modified-id=\"Bigram-spell-checker-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Bigram spell checker</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluating\" data-toc-modified-id=\"Evaluating-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Evaluating</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:05:29.633661Z",
     "start_time": "2021-04-16T10:05:29.614943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:01:03.661694Z",
     "start_time": "2021-04-22T14:01:03.658900Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import os,sys,inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir) \n",
    "\n",
    "import data\n",
    "from data import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:01:05.287729Z",
     "start_time": "2021-04-22T14:01:05.154316Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = utils.get_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:01:05.665220Z",
     "start_time": "2021-04-22T14:01:05.661919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': ['they', 'can', 'go', 'quite', 'farst'],\n",
       " 'corrected': ['they', 'can', 'go', 'quite', 'fast'],\n",
       " 'indexes': [4]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T13:41:02.620884Z",
     "start_time": "2021-04-22T13:41:02.617593Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr = [x['original'] for x in train]\n",
    "y_tr = [x['corrected'] for x in train]\n",
    "y_tr_idx =  [x['indexes'] for x in train]\n",
    "\n",
    "X_te = [x['original'] for x in test]\n",
    "y_te = [x['corrected'] for x in test]\n",
    "y_te_idx =  [x['indexes'] for x in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:15:31.905216Z",
     "start_time": "2021-04-22T16:15:31.870661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocabulary)=|3451\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "\n",
    "bigram_finder = BigramCollocationFinder.from_documents(X_tr)\n",
    "bigram_freq_dict = dict(bigram_finder.ngram_fd.items())\n",
    "list(bigram_freq_dict.keys())[0:10]\n",
    "\n",
    "import itertools\n",
    "vocabulary = set(list(itertools.chain(*bigram_freq_dict.keys())))\n",
    "print(f'len(vocabulary)=|{len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:15:41.704624Z",
     "start_time": "2021-04-22T16:15:41.668992Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('they', 'can'): 4,\n",
       " ('can', 'go'): 4,\n",
       " ('go', 'quite'): 1,\n",
       " ('quite', 'farst'): 1,\n",
       " ('this', 'was'): 3,\n",
       " ('was', 'a'): 34,\n",
       " ('a', 'Royl'): 3,\n",
       " ('Royl', 'Enfield'): 5,\n",
       " ('Enfield', 'Consulatoin'): 1,\n",
       " ('Consulatoin', '?'): 1,\n",
       " ('?', '_'): 19,\n",
       " ('there', 'were'): 9,\n",
       " ('were', 'some'): 4,\n",
       " ('some', 'Royl'): 1,\n",
       " ('Enfield', 'Racing'): 1,\n",
       " ('Racing', 'there'): 1,\n",
       " ('there', 'as'): 1,\n",
       " ('as', 'well'): 4,\n",
       " ('I', 'saw'): 2,\n",
       " ('saw', 'a'): 5,\n",
       " ('Enfield', 'come'): 1,\n",
       " ('come', 'first'): 1,\n",
       " ('first', 'it'): 1,\n",
       " ('it', 'was'): 51,\n",
       " ('was', 'like'): 1,\n",
       " ('like', 'mine'): 1,\n",
       " ('mine', '.'): 1,\n",
       " ('were', 'the'): 2,\n",
       " ('the', 'new'): 2,\n",
       " ('new', 'Japannese'): 1,\n",
       " ('Japannese', 'Hondor'): 1,\n",
       " ('they', 'are'): 1,\n",
       " ('are', 'very'): 1,\n",
       " ('very', 'farst'): 1,\n",
       " ('farst', 'and'): 1,\n",
       " ('and', 'geting'): 1,\n",
       " ('geting', 'quite'): 1,\n",
       " ('quite', 'popular'): 1,\n",
       " ('popular', 'in'): 1,\n",
       " ('in', 'England'): 3,\n",
       " ('saw', 'one'): 1,\n",
       " ('one', 'man'): 3,\n",
       " ('man', 'come'): 2,\n",
       " ('come', 'off'): 1,\n",
       " ('he', 'was'): 24,\n",
       " ('was', 'on'): 6,\n",
       " ('on', 'a'): 23,\n",
       " ('a', 'B.S.A'): 1,\n",
       " ('B.S.A', '.'): 2,\n",
       " ('One', 'man'): 1,\n",
       " ('man', 'had'): 1,\n",
       " ('had', 'to'): 11,\n",
       " ('to', 'go'): 29,\n",
       " ('go', 'in'): 4,\n",
       " ('in', 'Hospital'): 1,\n",
       " ('Hospital', 'because'): 1,\n",
       " ('because', 'he'): 5,\n",
       " ('he', 'broke'): 2,\n",
       " ('broke', 'is'): 1,\n",
       " ('is', 'lege'): 1,\n",
       " ('a_nouther', 'man'): 1,\n",
       " ('man', 'hearte'): 1,\n",
       " ('hearte', 'is'): 1,\n",
       " ('is', 'arm'): 1,\n",
       " ('arm', 'in'): 1,\n",
       " ('in', 'a'): 42,\n",
       " ('a', 'sidecar'): 1,\n",
       " ('sidecar', 'race'): 1,\n",
       " ('I', 'dont'): 7,\n",
       " ('dont', \"n't\"): 18,\n",
       " (\"n't\", 'thing'): 1,\n",
       " ('thing', 'it'): 2,\n",
       " ('was', 'much'): 1,\n",
       " ('I', 'sow'): 1,\n",
       " ('sow', 'a'): 2,\n",
       " ('a', 'side_car'): 1,\n",
       " ('side_car', 'turne'): 1,\n",
       " ('turne', 'right'): 1,\n",
       " ('right', 'over'): 1,\n",
       " ('the', 'side_care'): 1,\n",
       " ('side_care', 'man'): 1,\n",
       " ('man', 'was'): 2,\n",
       " ('was', 'alright'): 1,\n",
       " ('alright', 'but'): 1,\n",
       " ('but', 'the'): 7,\n",
       " ('the', 'motor_bike'): 1,\n",
       " ('motor_bike', 'Rider'): 1,\n",
       " ('Rider', 'was'): 1,\n",
       " ('was', 'thrown'): 2,\n",
       " ('thrown', 'into'): 1,\n",
       " ('into', 'the'): 15,\n",
       " ('the', 'crowd'): 2,\n",
       " (\"n't\", 'no'): 3,\n",
       " ('no', 'what'): 1,\n",
       " ('what', 'hapend'): 1,\n",
       " ('hapend', 'to'): 1,\n",
       " ('to', 'him'): 3,\n",
       " ('was', 'heart'): 1,\n",
       " ('heart', 'quite'): 1,\n",
       " ('quite', 'bad'): 1,\n",
       " ('bad', '.'): 2,\n",
       " ('the', 'sidecar'): 1,\n",
       " ('sidecar', 'broke'): 1,\n",
       " ('broke', 'off'): 1,\n",
       " ('off', 'the'): 2,\n",
       " ('the', 'moter_bike'): 2,\n",
       " ('moter_bike', 'and'): 1,\n",
       " ('and', 'spund'): 1,\n",
       " ('spund', 'down'): 1,\n",
       " ('down', 'the'): 30,\n",
       " ('the', 'track'): 4,\n",
       " ('track', 'for'): 1,\n",
       " ('for', '7'): 2,\n",
       " ('7', 'yards'): 1,\n",
       " ('yards', 'then'): 1,\n",
       " ('then', 'hit'): 1,\n",
       " ('hit', 'a'): 3,\n",
       " ('a', 'man'): 21,\n",
       " ('man', 'who'): 4,\n",
       " ('who', 'was'): 2,\n",
       " ('on', 'the'): 76,\n",
       " ('the', 'corner'): 5,\n",
       " ('corner', 'and'): 1,\n",
       " ('and', 'killed'): 1,\n",
       " ('killed', 'him'): 3,\n",
       " ('his', 'hat'): 1,\n",
       " ('hat', 'was'): 1,\n",
       " ('was', 'blowing'): 1,\n",
       " ('blowing', 'down'): 1,\n",
       " ('moter_bike', 'caugh'): 1,\n",
       " ('caugh', 'fire'): 1,\n",
       " ('fire', 'and'): 1,\n",
       " ('and', 'blow'): 1,\n",
       " ('blow', 'up'): 1,\n",
       " ('the', 'petal'): 1,\n",
       " ('petal', 'went'): 1,\n",
       " ('went', 'all'): 1,\n",
       " ('all', 'over'): 3,\n",
       " ('over', 'the'): 10,\n",
       " ('track', 'but'): 1,\n",
       " ('but', 'they'): 5,\n",
       " ('they', 'sone'): 1,\n",
       " ('sone', 'put'): 1,\n",
       " ('put', 'it'): 3,\n",
       " ('it', 'out'): 2,\n",
       " ('out', 'by'): 1,\n",
       " ('by', 'sand'): 1,\n",
       " ('there', 'was'): 23,\n",
       " ('was', 'some'): 1,\n",
       " ('some', 'oil'): 1,\n",
       " ('oil', 'got'): 1,\n",
       " ('got', 'on'): 5,\n",
       " ('track', 'as'): 1,\n",
       " ('it', 'made'): 2,\n",
       " ('made', 'it'): 2,\n",
       " ('it', 'very'): 6,\n",
       " ('very', 'slippery'): 1,\n",
       " ('They', 'were'): 4,\n",
       " ('were', 'quite'): 1,\n",
       " ('quite', 'a'): 4,\n",
       " ('a', 'few'): 17,\n",
       " ('few', 'people'): 1,\n",
       " ('people', 'there'): 1,\n",
       " ('there', 'because'): 2,\n",
       " ('because', 'it'): 5,\n",
       " ('a', 'nice'): 4,\n",
       " ('nice', 'dry'): 1,\n",
       " ('dry', 'day'): 1,\n",
       " ('The', 'fianl'): 1,\n",
       " ('fianl', 'race'): 1,\n",
       " ('race', 'was'): 1,\n",
       " ('were', '2'): 1,\n",
       " ('2', 'Royl'): 1,\n",
       " ('Enfield', '3'): 1,\n",
       " ('3', 'Japanese'): 1,\n",
       " ('Japanese', 'Hondor'): 2,\n",
       " ('Hondor', '1'): 1,\n",
       " ('1', 'B.S.A'): 1,\n",
       " ('3', 'Triumh'): 1,\n",
       " ('they', 'were'): 20,\n",
       " ('were', 'on'): 2,\n",
       " ('the', 'starting'): 2,\n",
       " ('starting', 'line'): 2,\n",
       " ('Bang', 'there'): 1,\n",
       " ('there', \"'re\"): 1,\n",
       " (\"'re\", 'off'): 1,\n",
       " ('one', 'BSA'): 1,\n",
       " ('BSA', 'is'): 1,\n",
       " ('is', 'still'): 2,\n",
       " ('still', 'on'): 3,\n",
       " ('line', 'the'): 1,\n",
       " ('the', 'rest'): 5,\n",
       " ('rest', 'are'): 1,\n",
       " ('are', 'round'): 1,\n",
       " ('round', 'the'): 8,\n",
       " ('the', 'bend'): 1,\n",
       " ('I', 'can'): 9,\n",
       " ('can', 'see'): 1,\n",
       " ('see', 'one'): 1,\n",
       " ('man', 'off'): 1,\n",
       " ('he', 'is'): 22,\n",
       " ('is', 'on'): 5,\n",
       " ('a', 'trumh'): 1,\n",
       " ('I', 'think'): 14,\n",
       " ('think', 'a'): 1,\n",
       " ('a', 'Japanese'): 1,\n",
       " ('Hondor', 'is'): 1,\n",
       " ('is', 'in'): 3,\n",
       " ('in', 'the'): 114,\n",
       " ('the', 'lead'): 1,\n",
       " ('think', 'he'): 1,\n",
       " ('is', 'go'): 1,\n",
       " ('go', 'to'): 21,\n",
       " ('to', 'lead'): 1,\n",
       " ('his', \"'s\"): 1,\n",
       " (\"'s\", 'one'): 2,\n",
       " ('one', 'by'): 1,\n",
       " ('by', '3'): 1,\n",
       " ('3', 'yards'): 1,\n",
       " ('yards', 'and'): 1,\n",
       " ('and', 'a'): 25,\n",
       " ('Enfield', '2'): 1,\n",
       " ('2', 'and'): 1,\n",
       " ('a', 'Hondor'): 1,\n",
       " ('Hondor', '3'): 1,\n",
       " ('3', '.'): 3,\n",
       " ('The', 'End'): 10,\n",
       " ('we', 'went'): 10,\n",
       " ('went', 'over'): 3,\n",
       " ('the', 'football'): 2,\n",
       " ('football', 'field'): 1,\n",
       " ('field', 'it'): 1,\n",
       " ('was', 'quite'): 4,\n",
       " ('quite', 'wet'): 1,\n",
       " ('wet', 'and'): 1,\n",
       " ('and', 'soon'): 2,\n",
       " ('soon', 'made'): 1,\n",
       " ('made', 'my'): 2,\n",
       " ('my', 'toes'): 1,\n",
       " ('toes', 'cold'): 1,\n",
       " ('cold', '.'): 2,\n",
       " ('the', 'grass'): 1,\n",
       " ('grass', 'was'): 1,\n",
       " ('quite', 'long'): 1,\n",
       " ('long', 'now'): 1,\n",
       " ('now', 'it'): 1,\n",
       " ('it', 'shon'): 1,\n",
       " ('shon', 'with'): 1,\n",
       " ('with', 'the'): 16,\n",
       " ('the', 'dew'): 2,\n",
       " ('dew', 'on'): 2,\n",
       " ('on', 'it'): 3,\n",
       " ('it', '.'): 26,\n",
       " ('a', 'field'): 2,\n",
       " ('field', 'ploud'): 1,\n",
       " ('ploud', 'next'): 1,\n",
       " ('next', 'to'): 1,\n",
       " ('to', 'it'): 6,\n",
       " ('it', 'I'): 4,\n",
       " ('think', 'it'): 3,\n",
       " ('was', 'don'): 1,\n",
       " ('don', 'yesterday'): 1,\n",
       " ('yesterday', '.'): 1,\n",
       " ('the', 'other'): 7,\n",
       " ('other', 'side'): 3,\n",
       " ('side', 'was'): 1,\n",
       " ('was', 'an'): 3,\n",
       " ('an', 'hedge'): 1,\n",
       " ('it', 'had'): 6,\n",
       " ('had', 'a'): 27,\n",
       " ('a', 'lot'): 13,\n",
       " ('lot', 'of'): 11,\n",
       " ('of', 'thorns'): 1,\n",
       " ('thorns', 'in'): 1,\n",
       " ('in', 'it'): 10,\n",
       " ('they', 'hade'): 1,\n",
       " ('hade', 'cleand'): 1,\n",
       " ('cleand', 'the'): 1,\n",
       " ('the', 'river'): 2,\n",
       " ('river', 'and'): 2,\n",
       " ('and', 'made'): 4,\n",
       " ('very', 'wide'): 1,\n",
       " ('wide', 'fore'): 1,\n",
       " ('fore', 'the'): 1,\n",
       " ('the', 'ducks'): 1,\n",
       " ('some', 'potato'): 1,\n",
       " ('potato', 'in'): 1,\n",
       " ('the', 'water'): 8,\n",
       " ('water', '.'): 2,\n",
       " ('some', 'brussels'): 1,\n",
       " ('brussels', 'men'): 1,\n",
       " ('men', 'sitting'): 1,\n",
       " ('sitting', 'in'): 3,\n",
       " ('the', 'parth_way'): 1,\n",
       " ('parth_way', 'round'): 1,\n",
       " ('round', 'a'): 2,\n",
       " ('a', 'fire'): 3,\n",
       " ('we', 'had'): 3,\n",
       " ('go', 'on'): 3,\n",
       " ('on', 'to'): 4,\n",
       " ('to', 'a'): 9,\n",
       " ('a', 'ploud'): 1,\n",
       " ('ploud', 'field'): 1,\n",
       " ('field', 'to'): 2,\n",
       " ('to', 'get'): 30,\n",
       " ('get', 'round'): 1,\n",
       " ('round', 'them'): 1,\n",
       " ('them', 'this'): 1,\n",
       " ('this', 'made'): 1,\n",
       " ('my', 'shoes'): 1,\n",
       " ('shoes', 'hevey'): 1,\n",
       " ('hevey', '.'): 1,\n",
       " ('quite', 'muddy'): 1,\n",
       " ('some', 'boys'): 1,\n",
       " ('boys', 'did'): 1,\n",
       " ('did', 'not'): 22,\n",
       " ('not', 'think'): 1,\n",
       " ('think', 'much'): 1,\n",
       " ('much', 'to'): 1,\n",
       " ('it', 'nor'): 1,\n",
       " ('nor', 'did'): 1,\n",
       " ('did', 'I'): 3,\n",
       " ('I', 'but'): 1,\n",
       " ('but', 'we'): 6,\n",
       " ('we', 'still'): 3,\n",
       " ('still', 'went'): 1,\n",
       " ('went', '.'): 4,\n",
       " ('the', 'brussels'): 1,\n",
       " ('brussels', 'leaves'): 1,\n",
       " ('leaves', 'made'): 1,\n",
       " ('made', 'them'): 2,\n",
       " ('them', 'look'): 1,\n",
       " ('look', 'pirls'): 1,\n",
       " ('pirls', '.'): 1,\n",
       " ('Mr', '.'): 10,\n",
       " ('.', 'Holbrook'): 3,\n",
       " ('Holbrook', 'went'): 1,\n",
       " ('the', 'brook'): 2,\n",
       " ('brook', 'first'): 1,\n",
       " ('some', 'were'): 1,\n",
       " ('were', 'boys'): 1,\n",
       " ('boys', 'going'): 1,\n",
       " ('going', 'round'): 2,\n",
       " ('round', 'but'): 1,\n",
       " ('but', 'only'): 1,\n",
       " ('only', 'one'): 1,\n",
       " ('one', 'went'): 1,\n",
       " ('after', 'all'): 1,\n",
       " ('all', 'the'): 11,\n",
       " ('rest', 'went'): 1,\n",
       " ('the', 'tree'): 5,\n",
       " ('tree', 'Gerald'): 1,\n",
       " ('Gerald', 'nealy'): 1,\n",
       " ('nealy', 'went'): 1,\n",
       " ('went', 'in'): 16,\n",
       " ('one', 'foot'): 1,\n",
       " ('foot', 'toch'): 1,\n",
       " ('toch', 'the'): 1,\n",
       " ('then', 'we'): 4,\n",
       " ('went', 'across'): 1,\n",
       " ('across', 'the'): 6,\n",
       " ('new', 'pice'): 1,\n",
       " ('pice', 'of'): 1,\n",
       " ('of', 'grass'): 1,\n",
       " ('grass', 'into'): 1,\n",
       " ('into', 'school'): 1,\n",
       " ('went', 'and'): 10,\n",
       " ('and', 'cleand'): 1,\n",
       " ('cleand', 'our'): 1,\n",
       " ('our', 'shoes'): 1,\n",
       " ('shoes', 'with'): 1,\n",
       " ('with', 'paper'): 1,\n",
       " ('then', 'the'): 4,\n",
       " ('the', 'bell'): 2,\n",
       " ('bell', 'went'): 1,\n",
       " ('went', 'for'): 4,\n",
       " ('for', 'brack'): 1,\n",
       " ('brack', '.'): 1,\n",
       " ('2', '.'): 2,\n",
       " ('JOAN', 'STALL'): 1,\n",
       " ('STALL', 'page'): 1,\n",
       " ('page', '54'): 1,\n",
       " ('A', 'poem'): 1,\n",
       " ('poem', 'A'): 1,\n",
       " ('A', 'little'): 1,\n",
       " ('little', 'yellow'): 2,\n",
       " ('yellow', 'bird'): 1,\n",
       " ('bird', 'sat'): 1,\n",
       " ('sat', 'on'): 6,\n",
       " ('on', 'my'): 5,\n",
       " ('my', 'window'): 1,\n",
       " ('window', 'sill'): 1,\n",
       " ('sill', 'He'): 1,\n",
       " ('He', 'hop'): 1,\n",
       " ('and', 'poped'): 1,\n",
       " ('poped', 'about'): 1,\n",
       " ('about', 'He'): 1,\n",
       " ('He', 'wisheld'): 1,\n",
       " ('wisheld', 'he'): 1,\n",
       " ('he', 'cherped'): 1,\n",
       " ('cherped', '.'): 1,\n",
       " ('I', 'trid'): 2,\n",
       " ('trid', 'to'): 2,\n",
       " ('to', 'chach'): 1,\n",
       " ('chach', 'my'): 1,\n",
       " ('my', 'little'): 2,\n",
       " ('yellow', 'brid'): 2,\n",
       " ('brid', 'but'): 1,\n",
       " ('but', 'he'): 9,\n",
       " ('he', 'flew'): 1,\n",
       " ('flew', 'in'): 1,\n",
       " ('in', 'to'): 4,\n",
       " ('to', 'the'): 96,\n",
       " ('the', 'golden'): 1,\n",
       " ('golden', 'yellow'): 1,\n",
       " ('yellow', 'sun'): 1,\n",
       " ('sun', ','): 2,\n",
       " (',', 'O'): 2,\n",
       " ('O', 'how'): 1,\n",
       " ('how', 'I'): 1,\n",
       " ('I', 'wish'): 1,\n",
       " ('wish', 'that'): 1,\n",
       " ('that', 'was'): 5,\n",
       " ('was', 'my'): 2,\n",
       " ('my', 'yellow'): 1,\n",
       " ('brid', '.'): 1,\n",
       " ('ANN', 'That'): 1,\n",
       " ('That', \"'s\"): 3,\n",
       " (\"'s\", 'what'): 3,\n",
       " ('what', 'you'): 1,\n",
       " ('you', 'think'): 4,\n",
       " ('think', 'JOHN'): 1,\n",
       " ('JOHN', 'That'): 1,\n",
       " ('what', 'I'): 4,\n",
       " ('I', 'no'): 2,\n",
       " ('no', '.'): 1,\n",
       " ('.', 'JEAN'): 1,\n",
       " ('JEAN', 'the'): 1,\n",
       " ('the', 'longer'): 1,\n",
       " ('longer', 'you'): 1,\n",
       " ('you', 'stay'): 2,\n",
       " ('stay', 'here'): 1,\n",
       " ('here', 'talking'): 1,\n",
       " ('talking', 'we'): 1,\n",
       " ('we', \"carn't\"): 1,\n",
       " (\"carn't\", \"n't\"): 1,\n",
       " (\"n't\", 'get'): 3,\n",
       " ('get', 'on'): 2,\n",
       " ('on', 'with'): 2,\n",
       " ('with', 'you'): 9,\n",
       " ('you', 'here'): 1,\n",
       " ('here', 'DENNIS'): 1,\n",
       " ('DENNIS', 'Come'): 1,\n",
       " ('Come', 'on'): 1,\n",
       " ('on', 'John'): 1,\n",
       " ('John', 'I'): 1,\n",
       " ('no', 'when'): 1,\n",
       " ('when', 'were'): 1,\n",
       " ('were', \"'re\"): 2,\n",
       " ('not', 'whanted'): 1,\n",
       " ('whanted', 'JEAN'): 1,\n",
       " ('JEAN', 'Yes'): 3,\n",
       " ('Yes', 'your'): 1,\n",
       " ('your', \"'re\"): 2,\n",
       " ('not', 'whated'): 1,\n",
       " ('whated', 'here'): 1,\n",
       " ('here', 'JOHN'): 1,\n",
       " ('JOHN', 'Good_by'): 1,\n",
       " ('ANN', 'I'): 1,\n",
       " ('I', 'knew'): 1,\n",
       " ('knew', 'that'): 1,\n",
       " ('that', 'would'): 1,\n",
       " ('would', 'happen'): 1,\n",
       " ('happen', '.'): 1,\n",
       " ('Apple', 'red'): 1,\n",
       " ('red', 'and'): 1,\n",
       " ('and', 'green'): 1,\n",
       " ('green', 'start'): 1,\n",
       " ('start', 'are'): 1,\n",
       " ('are', 'bright'): 2,\n",
       " ('bright', 'The'): 1,\n",
       " ('The', 'apple'): 2,\n",
       " ('apple', 'trees'): 1,\n",
       " ('trees', 'are'): 2,\n",
       " ('are', 'tall'): 1,\n",
       " ('tall', 'The'): 1,\n",
       " ('The', 'leavs'): 1,\n",
       " ('leavs', 'are'): 1,\n",
       " ('are', 'small'): 1,\n",
       " ('small', 'The'): 1,\n",
       " ('fall', 'in'): 1,\n",
       " ('in', 'Aount'): 1,\n",
       " ('Aount', 'time'): 1,\n",
       " ('When', 'yow'): 1,\n",
       " ('yow', 'get'): 1,\n",
       " ('get', 'to'): 7,\n",
       " ('the', 'corres'): 3,\n",
       " ('corres', 'inside'): 1,\n",
       " ('inside', 'the'): 3,\n",
       " ('corres', 'are'): 1,\n",
       " ('are', 'pips'): 1,\n",
       " ('pips', 'yow'): 1,\n",
       " ('yow', 'thorght'): 1,\n",
       " ('thorght', 'the'): 1,\n",
       " ('corres', 'on'): 1,\n",
       " ('the', 'fire'): 5,\n",
       " ('fire', 'to'): 1,\n",
       " ('to', 'brun'): 1,\n",
       " ('brun', 'then'): 1,\n",
       " ('then', '.'): 7,\n",
       " ('the', 'boys'): 4,\n",
       " ('boys', 'in'): 1,\n",
       " ('in', 'our'): 1,\n",
       " ('our', 'village'): 1,\n",
       " ('village', 'go'): 1,\n",
       " ('the', 'orcher'): 1,\n",
       " ('orcher', 'and'): 1,\n",
       " ('and', 'clim'): 1,\n",
       " ('clim', 'the'): 1,\n",
       " ('the', 'trees'): 3,\n",
       " ('trees', 'and'): 1,\n",
       " ('and', 'pinch'): 1,\n",
       " ('pinch', 'then'): 2,\n",
       " ('all', 'then'): 1,\n",
       " ('then', 'they'): 14,\n",
       " ('they', 'run'): 1,\n",
       " ('run', 'away'): 4,\n",
       " ('away', 'with'): 1,\n",
       " ('with', 'then'): 1,\n",
       " ('the', 'eyewigs'): 1,\n",
       " ('eyewigs', 'fall'): 1,\n",
       " ('fall', 'out'): 2,\n",
       " ('out', 'in'): 4,\n",
       " ('in', 'there'): 2,\n",
       " ('there', 'pocket'): 1,\n",
       " ('the', 'apple'): 1,\n",
       " ('apple', 'are'): 1,\n",
       " ('are', 'sweet'): 1,\n",
       " ('sweet', '.'): 2,\n",
       " ('are', 'fall'): 1,\n",
       " ('fall', 'of'): 1,\n",
       " ('of', 'apple'): 1,\n",
       " ('apple', '.'): 1,\n",
       " ('some', 'of'): 7,\n",
       " ('of', 'then'): 1,\n",
       " ('then', 'are'): 2,\n",
       " ('are', 'cookers'): 1,\n",
       " ('cookers', '.'): 1,\n",
       " ('the', 'apples'): 1,\n",
       " ('apples', 'are'): 1,\n",
       " ('are', 'so'): 1,\n",
       " ('so', 'gay'): 2,\n",
       " ('gay', 'haing'): 1,\n",
       " ('haing', 'on'): 1,\n",
       " ('trees', '.'): 2,\n",
       " ('the', 'sun'): 5,\n",
       " ('sun', 'on'): 1,\n",
       " ('on', 'then'): 1,\n",
       " ('no', 'wonder'): 2,\n",
       " ('wonder', 'the'): 1,\n",
       " ('boys', 'like'): 1,\n",
       " ('like', 'pinch'): 1,\n",
       " ('What', 'I'): 1,\n",
       " ('I', 'am'): 55,\n",
       " ('am', 'going'): 17,\n",
       " ('going', 'to'): 20,\n",
       " ('to', 'do'): 14,\n",
       " ('do', 'when'): 1,\n",
       " ('when', 'I'): 15,\n",
       " ('I', 'leave'): 11,\n",
       " ('leave', 'school'): 12,\n",
       " ('When', 'I'): 10,\n",
       " ('leave', 'I'): 1,\n",
       " ('I', 'whant'): 4,\n",
       " ('whant', 'to'): 3,\n",
       " ('to', 'be'): 16,\n",
       " ('be', 'a'): 5,\n",
       " ('a', 'hair_dreser'): 1,\n",
       " ('hair_dreser', 'in'): 1,\n",
       " ('in', 'Melton'): 4,\n",
       " ('Melton', 'or'): 1,\n",
       " ('or', 'Tonbury'): 1,\n",
       " ('Tonbury', '.'): 1,\n",
       " ('or', 'I'): 1,\n",
       " ('to', 'work'): 13,\n",
       " ('work', 'in'): 5,\n",
       " ('a', 'shop'): 1,\n",
       " ('shop', '.'): 2,\n",
       " ('I', 'do'): 11,\n",
       " ('do', 'not'): 9,\n",
       " ('not', 'whont'): 1,\n",
       " ('whont', 'to'): 4,\n",
       " ('get', 'marryid'): 1,\n",
       " ('marryid', 'till'): 1,\n",
       " ('till', 'I'): 1,\n",
       " ('am', 'twentone'): 1,\n",
       " ('twentone', '.'): 1,\n",
       " ('I', 'have'): 35,\n",
       " ('have', 'children'): 1,\n",
       " ('children', 'I'): 4,\n",
       " ('to', 'leave'): 2,\n",
       " ('leave', 'work'): 1,\n",
       " ('work', 'and'): 3,\n",
       " ('and', 'the'): 91,\n",
       " ('the', 'husbon'): 1,\n",
       " ('husbon', 'has'): 1,\n",
       " ('has', 'got'): 4,\n",
       " ('got', 'to'): 8,\n",
       " ('to', 'like'): 2,\n",
       " ('like', 'children'): 1,\n",
       " ('children', 'and'): 3,\n",
       " ('and', 'tack'): 1,\n",
       " ('tack', 'them'): 1,\n",
       " ('them', 'out'): 2,\n",
       " ('out', '.'): 5,\n",
       " ('whant', 'twins'): 1,\n",
       " ('twins', '.'): 2,\n",
       " ('and', 'I'): 33,\n",
       " ('whant', 'four'): 1,\n",
       " ('four', 'bridesmaids'): 1,\n",
       " ('bridesmaids', '.'): 1,\n",
       " ('I', 'whont'): 1,\n",
       " ('to', 'live'): 2,\n",
       " ('live', 'in'): 4,\n",
       " ('a', 'bungalow'): 3,\n",
       " ('bungalow', '.'): 1,\n",
       " ('for', 'my'): 3,\n",
       " ('my', 'holiday'): 2,\n",
       " ('holiday', 'I'): 2,\n",
       " ('I', 'would'): 16,\n",
       " ('would', 'like'): 9,\n",
       " ('like', 'to'): 11,\n",
       " ('to', 'butlins'): 1,\n",
       " ('butlins', ','): 1,\n",
       " (',', 'for'): 2,\n",
       " ('for', 'a'): 31,\n",
       " ('a', 'hunemoon'): 1,\n",
       " ('hunemoon', 'I'): 1,\n",
       " ('I', 'like'): 11,\n",
       " ('to', 'Scotland'): 1,\n",
       " ('Scotland', '.'): 1,\n",
       " ('On', 'our'): 1,\n",
       " ('our', 'table'): 1,\n",
       " ('table', 'there'): 1,\n",
       " ('there', 'is'): 4,\n",
       " ('is', 'a'): 34,\n",
       " ('a', 'sculptor'): 1,\n",
       " ('sculptor', 'modal'): 1,\n",
       " ('modal', 'this'): 1,\n",
       " ('was', 'made'): 2,\n",
       " ('made', 'by'): 1,\n",
       " ('by', 'Mrs'): 1,\n",
       " ('Mrs', 'Chalmers'): 1,\n",
       " ('Chalmers', 'This'): 1,\n",
       " ('This', 'is'): 3,\n",
       " ('is', 'Just'): 1,\n",
       " ('Just', 'like'): 1,\n",
       " ('like', 'a'): 11,\n",
       " ('a', 'real'): 1,\n",
       " ('real', 'think'): 1,\n",
       " ('think', '.'): 1,\n",
       " ('this', 'is'): 5,\n",
       " ('a', 'modal'): 1,\n",
       " ('modal', 'of'): 1,\n",
       " ('of', 'a'): 8,\n",
       " ('a', 'boy'): 12,\n",
       " ('boy', 'it'): 1,\n",
       " ('it', 'is'): 15,\n",
       " ('is', 'staning'): 1,\n",
       " ('staning', 'on'): 1,\n",
       " ('a', 'block'): 1,\n",
       " ('block', 'wood'): 1,\n",
       " ('wood', 'This'): 1,\n",
       " ('This', 'was'): 1,\n",
       " ('was', 'malod'): 1,\n",
       " ('malod', 'in'): 1,\n",
       " ('in', 'clay'): 1,\n",
       " ('clay', 'not'): 1,\n",
       " ('not', 'a'): 7,\n",
       " ('a', 'whole'): 1,\n",
       " ('whole', 'molad'): 1,\n",
       " ('molad', 'Just'): 1,\n",
       " ('Just', 'the'): 1,\n",
       " ('the', 'Head'): 1,\n",
       " ('Head', '.'): 1,\n",
       " ('Swan', 'upping'): 1,\n",
       " ('When', 'they'): 5,\n",
       " ('they', 'do'): 2,\n",
       " ('do', 'this'): 1,\n",
       " ('this', 'kind'): 1,\n",
       " ('kind', 'of'): 2,\n",
       " ('of', 'think'): 1,\n",
       " ('think', 'they'): 1,\n",
       " ('they', 'need'): 1,\n",
       " ('need', 'comer'): 1,\n",
       " ('comer', 'waters'): 1,\n",
       " ('waters', '.'): 1,\n",
       " ('they', 'have'): 5,\n",
       " ('have', 'to'): 19,\n",
       " ('to', 'handle'): 1,\n",
       " ('handle', 'them'): 1,\n",
       " ('them', 'very'): 1,\n",
       " ('very', 'caresll'): 1,\n",
       " ('caresll', 'They'): 1,\n",
       " ('They', 'use'): 1,\n",
       " ('use', 'small'): 1,\n",
       " ('small', 'Boats'): 1,\n",
       " ('Boats', 'peolpe'): 1,\n",
       " ('peolpe', 'think'): 1,\n",
       " ('think', 'this'): 1,\n",
       " ('is', 'very'): 4,\n",
       " ('very', 'corller'): 1,\n",
       " ('corller', 'to'): 1,\n",
       " ('to', 'cash'): 1,\n",
       " ('cash', 'them'): 1,\n",
       " ('them', 'and'): 2,\n",
       " ('and', 'put'): 7,\n",
       " ('put', 'stamp'): 1,\n",
       " ('stamp', 'on'): 1,\n",
       " ('on', 'there'): 2,\n",
       " ('there', 'wings'): 2,\n",
       " ('wings', ','): 1,\n",
       " (',', 'in'): 2,\n",
       " ('in', 'this'): 3,\n",
       " ('this', 'picture'): 2,\n",
       " ('picture', 'there'): 1,\n",
       " ('man', 'carring'): 1,\n",
       " ('carring', 'on'): 1,\n",
       " ('on', 'out'): 1,\n",
       " ('out', 'of'): 38,\n",
       " ('of', 'the'): 87,\n",
       " ('the', 'Boat'): 1,\n",
       " ('Boat', 'its'): 1,\n",
       " ('its', 'Wing'): 1,\n",
       " ('Wing', 'are'): 1,\n",
       " ('are', 'tied'): 1,\n",
       " ('tied', 'so'): 1,\n",
       " ('so', 'that'): 6,\n",
       " ('that', 'it'): 2,\n",
       " ('it', 'can'): 1,\n",
       " ('can', 'not'): 3,\n",
       " ('not', 'brick'): 1,\n",
       " ('brick', 'the'): 1,\n",
       " ('the', 'mans'): 1,\n",
       " ('mans', \"'s\"): 2,\n",
       " (\"'s\", 'arms'): 1,\n",
       " ('arms', ','): 1,\n",
       " (',', 'the'): 13,\n",
       " ('the', 'swan'): 1,\n",
       " ('swan', 'are'): 1,\n",
       " ('are', 'dellitent'): 1,\n",
       " ('dellitent', 'think'): 1,\n",
       " ('one', 'day'): 6,\n",
       " ('day', 'the'): 3,\n",
       " ('sun', 'was'): 2,\n",
       " ('was', 'out'): 2,\n",
       " ('out', 'and'): 11,\n",
       " ('and', 'up'): 2,\n",
       " ('up', 'the'): 25,\n",
       " ('the', 'stairs'): 3,\n",
       " ('stairs', 'I'): 1,\n",
       " ('I', 'went'): 27,\n",
       " ('My', 'mother'): 3,\n",
       " ('mother', 'in'): 1,\n",
       " ('a', 'temper'): 1,\n",
       " ('temper', ','): 1,\n",
       " (',', 'I'): 27,\n",
       " ('I', 'spent'): 1,\n",
       " ('spent', 'my'): 1,\n",
       " ('my', 'day'): 1,\n",
       " ('day', 'in'): 3,\n",
       " ('in', 'bed'): 7,\n",
       " ('bed', '.'): 2,\n",
       " ('for', 'fear'): 1,\n",
       " ('fear', 'of'): 1,\n",
       " ('of', 'my'): 6,\n",
       " ('my', 'mother'): 13,\n",
       " ('mother', 'I'): 1,\n",
       " ('I', 'darnt'): 1,\n",
       " ('darnt', \"n't\"): 1,\n",
       " (\"n't\", 'go'): 2,\n",
       " ('go', 'near'): 1,\n",
       " ('near', '.'): 1,\n",
       " ('For', 'fear'): 1,\n",
       " ('fear', 'I'): 1,\n",
       " ('I', 'dident'): 1,\n",
       " ('dident', \"n't\"): 2,\n",
       " ('to', 'see'): 13,\n",
       " ('see', 'the'): 14,\n",
       " ('There', 'she'): 1,\n",
       " ('she', 'lays'): 1,\n",
       " ('lays', 'So'): 1,\n",
       " ('So', 'quitely'): 1,\n",
       " ('quitely', 'there'): 1,\n",
       " ('there', 'Waiting'): 1,\n",
       " ('Waiting', 'for'): 1,\n",
       " ('for', 'the'): 19,\n",
       " ('the', 'day'): 3,\n",
       " ('day', 'People'): 1,\n",
       " ('People', 'weeping'): 1,\n",
       " ('weeping', 'waiting'): 1,\n",
       " ('waiting', 'Sighing'): 1,\n",
       " ('Sighing', 'all'): 1,\n",
       " ('the', 'days'): 1,\n",
       " ('days', 'There'): 1,\n",
       " ('There', 'hearts'): 1,\n",
       " ('hearts', 'are'): 1,\n",
       " ('are', 'weeping'): 1,\n",
       " ('weeping', 'Crying'): 1,\n",
       " ('Crying', 'so'): 1,\n",
       " ('so', 'Hearts'): 1,\n",
       " ('Hearts', 'beting'): 1,\n",
       " ('beting', 'so'): 1,\n",
       " ('so', 'raperly'): 1,\n",
       " ('MARRY', 'Linda'): 1,\n",
       " ('Linda', 'stop'): 1,\n",
       " ('stop', 'it'): 1,\n",
       " ('it', 'and'): 4,\n",
       " ('and', 'come'): 6,\n",
       " ('come', 'out'): 5,\n",
       " ('out', 'hear'): 1,\n",
       " ('hear', 'at'): 1,\n",
       " ('at', 'once'): 1,\n",
       " ('once', '.'): 1,\n",
       " ('JOHN', 'What'): 2,\n",
       " ('What', \"'s\"): 2,\n",
       " (\"'s\", 'wong'): 1,\n",
       " ('wong', '.'): 1,\n",
       " ('MARRY', 'I'): 1,\n",
       " ('I', 'told'): 3,\n",
       " ('told', 'her'): 4,\n",
       " ('her', 'that'): 1,\n",
       " ('that', 'you'): 3,\n",
       " ('you', 'were'): 1,\n",
       " ('were', 'thaing'): 1,\n",
       " ('thaing', 'me'): 1,\n",
       " ('me', 'owt'): 1,\n",
       " ('owt', 'at'): 1,\n",
       " ('at', 'half'): 2,\n",
       " ('half', 'past'): 3,\n",
       " ('past', '12'): 1,\n",
       " ('12', 'to_night'): 1,\n",
       " ('to_night', 'JOHN'): 1,\n",
       " ('JOHN', 'You'): 1,\n",
       " ('You', 'what'): 1,\n",
       " ('what', '.'): 1,\n",
       " ('MARRY', 'Well'): 1,\n",
       " ('Well', 'I'): 4,\n",
       " ('I', 'had'): 11,\n",
       " ('to', 'tell'): 4,\n",
       " ('tell', 'her'): 2,\n",
       " ('her', 'JOHN'): 1,\n",
       " ('JOHN', 'Yes'): 1,\n",
       " ('Yes', 'I'): 2,\n",
       " ('I', 'suppose'): 1,\n",
       " ('suppose', 'you'): 1,\n",
       " ('you', 'did'): 1,\n",
       " ('did', 'well'): 1,\n",
       " ('well', 'it'): 1,\n",
       " ('it', \"'s\"): 1,\n",
       " (\"'s\", 'not'): 3,\n",
       " ('not', 'her'): 2,\n",
       " ('her', 'falt'): 1,\n",
       " ('falt', '.'): 1,\n",
       " ('its', \"'s\"): 10,\n",
       " (\"'s\", 'my'): 1,\n",
       " ('my', 'falt'): 1,\n",
       " ('falt', 'for'): 1,\n",
       " ('for', 'traing'): 1,\n",
       " ('traing', 'to'): 1,\n",
       " ('a', 'pig'): 1,\n",
       " ('pig', '.'): 1,\n",
       " ('MARRY', 'mine'): 1,\n",
       " ('mine', 'too'): 1,\n",
       " ('too', ','): 2,\n",
       " (',', 'still'): 1,\n",
       " ('still', 'you'): 1,\n",
       " ('you', \"'ve\"): 1,\n",
       " (\"'ve\", 'got'): 2,\n",
       " ('got', 'the'): 11,\n",
       " ('the', 'job'): 2,\n",
       " ('job', 'of'): 1,\n",
       " ('of', 'ching'): 1,\n",
       " ('ching', 'her'): 1,\n",
       " ('her', 'or'): 1,\n",
       " ('or', 'me'): 1,\n",
       " ('me', ','): 9,\n",
       " (',', 'JOHN'): 1,\n",
       " ('JOHN', 'I'): 2,\n",
       " ('think', 'I'): 5,\n",
       " ('had', 'bet'): 1,\n",
       " ('bet', 'go'): 1,\n",
       " ('go', 'home'): 3,\n",
       " ('home', 'and'): 8,\n",
       " ('and', 'think'): 1,\n",
       " ('out', 'LINDA'): 1,\n",
       " ('LINDA', 'theres'): 1,\n",
       " ('theres', \"'s\"): 4,\n",
       " (\"'s\", 'nothing'): 3,\n",
       " ('nothing', 'to'): 4,\n",
       " ('to', 'think'): 3,\n",
       " ('think', 'out'): 1,\n",
       " ('out', 'I'): 1,\n",
       " ('am', 'through'): 1,\n",
       " ('through', 'with'): 1,\n",
       " ('you', 'for'): 1,\n",
       " ('for', 'good'): 1,\n",
       " ('good', '.'): 4,\n",
       " ('JOHN', 'Linda'): 1,\n",
       " ('Linda', 'wait'): 1,\n",
       " ('wait', 'LINDA'): 1,\n",
       " ('LINDA', 'What'): 1,\n",
       " (\"'s\", 'the'): 5,\n",
       " ('the', 'good'): 1,\n",
       " ('good', 'of'): 1,\n",
       " ('of', 'waiting'): 1,\n",
       " ('waiting', 'for'): 6,\n",
       " ('for', 'you'): 4,\n",
       " ('you', 'is'): 1,\n",
       " ('is', 'it'): 4,\n",
       " ('Linda', 'slander'): 1,\n",
       " ('slander', 'the'): 1,\n",
       " ('the', 'door'): 21,\n",
       " ('door', 'in'): 1,\n",
       " ('in', 'his'): 6,\n",
       " ('his', 'face'): 1,\n",
       " ('face', '.'): 2,\n",
       " ('JOHN', 'hello'): 1,\n",
       " ('hello', 'Jean'): 1,\n",
       " ('Jean', 'carnt'): 1,\n",
       " ('carnt', \"n't\"): 1,\n",
       " (\"n't\", 'we'): 2,\n",
       " ('we', 'tallk'): 1,\n",
       " ('tallk', 'it'): 1,\n",
       " ('it', 'over'): 3,\n",
       " ('over', 'and'): 3,\n",
       " ('come', 'to'): 6,\n",
       " ('to', 'some'): 1,\n",
       " ('some', 'disson'): 1,\n",
       " ('JEAN', 'Talk'): 1,\n",
       " ('Talk', 'about'): 1,\n",
       " ('about', 'what'): 2,\n",
       " ('what', 'for'): 1,\n",
       " ('for', 'instead'): 1,\n",
       " ('JOHN', 'Well'): 2,\n",
       " ('Well', 'make'): 1,\n",
       " ('make', 'it'): 4,\n",
       " ('it', 'up'): 5,\n",
       " ('up', '.'): 10,\n",
       " ('JEAN', 'I'): 2,\n",
       " ('am', 'sorry'): 1,\n",
       " ('sorry', 'John'): 1,\n",
       " ('John', '.'): 4,\n",
       " ('.', 'theres'): 1,\n",
       " ('to', 'make'): 4,\n",
       " ('make', 'up'): 2,\n",
       " ('What', 'do'): 1,\n",
       " ('do', 'you'): 8,\n",
       " ('you', 'mean'): 1,\n",
       " ('mean', ';'): 1,\n",
       " (';', 'There'): 1,\n",
       " ('There', \"'s\"): 1,\n",
       " (\"'s\", 'on'): 1,\n",
       " ('on', 'one'): 2,\n",
       " ('one', 'eles'): 1,\n",
       " ('eles', 'us'): 1,\n",
       " ('us', 'is'): 1,\n",
       " ('is', 'there'): 1,\n",
       " ('there', '.'): 3,\n",
       " ('JEAN', 'Well'): 2,\n",
       " ('Well', 'what'): 1,\n",
       " ('what', 'if'): 1,\n",
       " ('if', 'there'): 2,\n",
       " ('there', 'us'): 1,\n",
       " ('us', 'its'): 1,\n",
       " (\"'s\", 'got'): 3,\n",
       " ('got', 'nothing'): 1,\n",
       " ('do', 'with'): 1,\n",
       " ('JOHN', 'look'): 1,\n",
       " ('look', 'I'): 1,\n",
       " ('am', 'only'): 1,\n",
       " ('only', 'trining'): 1,\n",
       " ('trining', 'to'): 1,\n",
       " ('be', 'nice'): 1,\n",
       " ('JEAN', 'if'): 1,\n",
       " ('if', 'you'): 9,\n",
       " ('you', 'lay'): 1,\n",
       " ('lay', 'a'): 1,\n",
       " ('a', 'hand'): 1,\n",
       " ('hand', 'on'): 1,\n",
       " ('on', 'me'): 2,\n",
       " ('me', 'I'): 1,\n",
       " ('I', 'will'): 34,\n",
       " ('will', 'scem'): 1,\n",
       " ('JOHN', 'do'): 1,\n",
       " ('think', 'that'): 1,\n",
       " ('that', 'scards'): 1,\n",
       " ('scards', 'me'): 1,\n",
       " ('yow', 'just'): 1,\n",
       " ('just', 'whant'): 1,\n",
       " ...}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq_dict = dict(bigram_finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:49:21.210376Z",
     "start_time": "2021-04-22T16:49:21.208612Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:48:53.832447Z",
     "start_time": "2021-04-22T16:48:53.829269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'man', 'went'), ('man', 'went', 'to'), ('went', 'to', 'the')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in nltk.collocations.ngrams(['the','man','went','to','the'],n=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:50:11.020651Z",
     "start_time": "2021-04-22T16:50:11.014718Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "corrected_sentences = [train[n]['corrected'] for n in range(len(train))]\n",
    "corrected_words = [word.lower() for sentence in corrected_sentences for word in sentence]\n",
    "unique_corrected_words = set(corrected_words)\n",
    "n_total_words = len(corrected_words)\n",
    "vocabulary = unique_corrected_words\n",
    "\n",
    "def build_unigrams(corrected_words):\n",
    "    return Counter(corrected_words) \n",
    "\n",
    "def prob(word, unigrams, n_total_words):\n",
    "    word = word.lower()\n",
    "    word_counts = unigrams[word]\n",
    "    return word_counts / n_total_words\n",
    "\n",
    "# Test your code with the following\n",
    "# assert(unigram(\"me\")==87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:50:31.481956Z",
     "start_time": "2021-04-22T16:50:31.478873Z"
    }
   },
   "outputs": [],
   "source": [
    "unigrams = build_unigrams(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:11:48.101657Z",
     "start_time": "2021-04-22T14:11:48.098852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002200926223118896\n",
      "0.003989178779402999\n",
      "0.00018341051859324133\n"
     ]
    }
   ],
   "source": [
    "print(prob('house', unigrams, n_total_words))\n",
    "print(prob('me', unigrams, n_total_words))\n",
    "print(prob('television', unigrams, n_total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:59:08.180179Z",
     "start_time": "2021-04-22T15:59:08.177048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 87, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams['house'], unigrams['me'], unigrams['television']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T13:41:09.348370Z",
     "start_time": "2021-04-22T13:41:09.346422Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:55:25.027687Z",
     "start_time": "2021-04-22T15:55:25.025333Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_candidates(token, vocabulary):\n",
    "    # Write your code here.\n",
    "    distance_token_to_words = {word:edit_distance(word,token.lower()) for word in vocabulary}\n",
    "    minimum_distance = min(distance_token_to_words.values())\n",
    "    return sorted([word for word, distance in distance_token_to_words.items() \n",
    "                   if distance == minimum_distance], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:55:25.968831Z",
     "start_time": "2021-04-22T15:55:25.890697Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test your code as follows\n",
    "assert get_candidates(\"minde\",vocabulary) == ['mine', 'mind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:55:26.371682Z",
     "start_time": "2021-04-22T15:55:26.307029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boys', 'box', 'boss', 'bos']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates(\"boxs\",vocabulary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T13:41:38.321716Z",
     "start_time": "2021-04-22T13:41:35.495079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.6 ms ± 270 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit get_candidates(\"min\",vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram spell checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T13:41:39.792882Z",
     "start_time": "2021-04-22T13:41:39.789961Z"
    }
   },
   "outputs": [],
   "source": [
    "def correct_tokens(tokenized_sentence, unigrams, n_total_words):\n",
    "    tokenized_sentence = tokenized_sentence.copy()\n",
    "    \n",
    "    for index,word in enumerate(tokenized_sentence):\n",
    "        if (word and word.lower()) not in unique_corrected_words:\n",
    "            candidates = {candidate:prob(candidate, unigrams, n_total_words) for candidate in get_candidates(word,vocabulary)}\n",
    "            best_candidate  = max(candidates, key=candidates.get)\n",
    "            tokenized_sentence[index] = best_candidate\n",
    "\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T13:41:40.071568Z",
     "start_time": "2021-04-22T13:41:40.014980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'white', 'cat']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_tokens(tokenized_sentence = [\"this\", \"whitr\", \"cat\"],  \n",
    "               unigrams=unigrams, \n",
    "               n_total_words=n_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T13:41:40.949834Z",
     "start_time": "2021-04-22T13:41:40.903374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'my', 'cat']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_tokens(tokenized_sentence = [\"this\",\"is\",\"my\",\"caat\"],  \n",
    "               unigrams=unigrams, \n",
    "               n_total_words=n_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T13:41:41.158171Z",
     "start_time": "2021-04-22T13:41:41.102011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Joan', 'likes', 'pigs', 'and', 'cats']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_tokens(tokenized_sentence = [\"Joan\",\"likes\",\"pissa\",\"and\",\"cats\"],  \n",
    "               unigrams=unigrams, \n",
    "               n_total_words=n_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T13:41:42.043196Z",
     "start_time": "2021-04-22T13:41:42.040165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'pizza' in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:07:46.165989Z",
     "start_time": "2021-04-16T10:07:46.147178Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(test,  unigrams, n_total_words):\n",
    "    # Write your code here\n",
    "    count_total_words = 0\n",
    "    count_corrected_words = 0\n",
    "    for sentence in test:\n",
    "        corrected_sentence = correct_tokens(sentence['original'], unigrams, n_total_words)  \n",
    "        count_total_words +=len(sentence['corrected'])\n",
    "        count_corrected_words += sum(corrected_sentence[n] == sentence['corrected'][n] \n",
    "                                     for n in range(len(sentence['corrected'])))\n",
    "    return count_corrected_words/count_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:07:56.201531Z",
     "start_time": "2021-04-16T10:07:46.563698Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8380281690140845"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test, unigrams, n_total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram spell checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:08:16.461754Z",
     "start_time": "2021-04-16T10:08:16.410003Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from collections import Counter\n",
    "\n",
    "corrected_sentences = [train[n]['corrected'] for n in range(len(train))]\n",
    "corrected_words = [word.lower() for sentence in corrected_sentences for word in sentence]\n",
    "unique_corrected_words = set(corrected_words)\n",
    "finder = BigramCollocationFinder.from_words(corrected_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:08:17.160524Z",
     "start_time": "2021-04-16T10:08:17.141192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they', 'can', 'go', 'quite', 'fast', 'this', 'was', 'a', 'royal', 'enfield']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:08:20.807410Z",
     "start_time": "2021-04-16T10:08:20.786467Z"
    }
   },
   "outputs": [],
   "source": [
    "unigram_freq_dict = build_unigrams(corrected_words)\n",
    "bigram_freq_dict  = dict(finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:08:23.797604Z",
     "start_time": "2021-04-16T10:08:23.778754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they', 'can', 'go', 'quite', 'fast', 'this', 'was', 'a', 'royal', 'enfield']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:32.849824Z",
     "start_time": "2021-04-22T16:03:32.844421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('they', 'can'), 4),\n",
       " (('can', 'go'), 4),\n",
       " (('go', 'quite'), 1),\n",
       " (('quite', 'farst'), 1),\n",
       " (('this', 'was'), 3),\n",
       " (('was', 'a'), 34),\n",
       " (('a', 'Royl'), 3),\n",
       " (('Royl', 'Enfield'), 5),\n",
       " (('Enfield', 'Consulatoin'), 1),\n",
       " (('Consulatoin', '?'), 1)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools \n",
    "def top_k_dict(d, top_k = 10):\n",
    "    return [(x,bigram_freq_dict[x]) for k,x in enumerate(bigram_freq_dict) if k<top_k]\n",
    "\n",
    "top_k_dict(bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can notice that actually the bigrams where not computed correctly because we concatenated all the data from the  corrected sentences into a single list `corrected_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:33.365105Z",
     "start_time": "2021-04-22T16:03:33.361297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': ['they', 'can', 'go', 'quite', 'farst'],\n",
       " 'corrected': ['they', 'can', 'go', 'quite', 'fast'],\n",
       " 'indexes': [4]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:33.566602Z",
     "start_time": "2021-04-22T16:03:33.563133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': ['this', 'was', 'a', 'Royl', 'Enfield', 'Consulatoin', '?', '_'],\n",
       " 'corrected': ['this', 'was', 'a', 'Royal', 'Enfield', '_', '?', '_'],\n",
       " 'indexes': [3, 5]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how the bigram `(fast,this)` appears in the `bigram_freq_dict` but in reality this might not even be in the training data.\n",
    "\n",
    "It is a consequence of concatenating the training dat into a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:33.998066Z",
     "start_time": "2021-04-22T16:03:33.994242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('they', 'can'),\n",
       " ('can', 'go'),\n",
       " ('go', 'quite'),\n",
       " ('quite', 'farst'),\n",
       " ('this', 'was'),\n",
       " ('was', 'a')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigram_freq_dict)[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could build the bigrams avoiding this issue using `BigramCollocationFinder.from_documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:34.444858Z",
     "start_time": "2021-04-22T16:03:34.440979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(('they', 'can'), 1), (('can', 'go'), 1), (('go', 'quite'), 1), (('quite', 'fast'), 1), (('this', 'was'), 1), (('was', 'a'), 1), (('a', 'Royal'), 1), (('Royal', 'Enfield'), 1), (('Enfield', '_'), 1), (('_', '?'), 1), (('?', '_'), 1)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_finder = BigramCollocationFinder.from_documents([['they', 'can', 'go', 'quite', 'fast'],\n",
    "                                                         ['this', 'was', 'a', 'Royal', 'Enfield', '_', '?', '_']])\n",
    "bigram_freq_dict_aux = bigram_finder.ngram_fd.items()\n",
    "bigram_freq_dict_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build again `bigram_freq_dict_aux` using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:39.289972Z",
     "start_time": "2021-04-22T16:03:39.256197Z"
    }
   },
   "outputs": [],
   "source": [
    "corrected_sentences = [train[n]['corrected'] for n in range(len(train))]\n",
    "finder = BigramCollocationFinder.from_documents(corrected_sentences)\n",
    "bigram_freq_dict = dict(finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:40.248460Z",
     "start_time": "2021-04-22T16:03:40.245825Z"
    }
   },
   "outputs": [],
   "source": [
    "def prob_word(word, word_to_count, n_total_words, n_vocabulary):\n",
    "    # Write your code here.\n",
    "    word = word.lower()\n",
    "    word_counts = word_to_count[word]\n",
    "    return word_counts / n_total_words\n",
    "\n",
    "#def prob_word(word, word_to_count, n_total_words, n_vocabulary): \n",
    "#    return (word_to_count[word]+1) / (n_total_words+ n_vocabulary)\n",
    "\n",
    "def bigrams_starting_by(word, bigram_freq_dictionary): \n",
    "    return [t for t in list(bigram_freq_dict.keys()) if t[0] == word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:40.518497Z",
     "start_time": "2021-04-22T16:03:40.516383Z"
    }
   },
   "outputs": [],
   "source": [
    "n_total_words, len(unique_corrected_words)\n",
    "n_vocabulary = len(unique_corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:41.431569Z",
     "start_time": "2021-04-22T16:03:41.427241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 'see'),\n",
       " ('dog', 'and'),\n",
       " ('dog', 'run'),\n",
       " ('dog', 'is'),\n",
       " ('dog', ','),\n",
       " ('dog', 'had'),\n",
       " ('dog', 'for'),\n",
       " ('dog', 'Toby'),\n",
       " ('dog', '.'),\n",
       " ('dog', 'it')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_starting_by('dog', bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:41.958640Z",
     "start_time": "2021-04-22T16:03:41.955964Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_dictionary_value(bigram, bigram_freq_dict):\n",
    "    try:\n",
    "        return bigram_freq_dict[bigram]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def count_bigrams(list_bigrams, bigram_freq_dict): \n",
    "    return sum([return_dictionary_value(bigram, bigram_freq_dict) for bigram in list_bigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:42:08.475097Z",
     "start_time": "2021-04-22T16:42:08.472290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_bigrams([('they','can')], bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:45:12.452805Z",
     "start_time": "2021-04-22T16:45:12.450361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13054"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:42:10.004696Z",
     "start_time": "2021-04-22T16:42:10.002384Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def probability_bigram(word1, word2, bigram_freq_dict):\n",
    "    if count_bigrams([(word1,word2)], bigram_freq_dict) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return count_bigrams([(word1,word2)], bigram_freq_dict)/count_bigrams(bigrams_starting_by(word1,bigram_freq_dict), bigram_freq_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:42:10.444741Z",
     "start_time": "2021-04-22T16:42:10.440964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090909090909091"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('dog','is')\n",
    "probability_bigram(word1,word2, bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:42:11.007228Z",
     "start_time": "2021-04-22T16:42:11.003592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12686567164179105"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('was','a')\n",
    "probability_bigram(word1,word2, bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:42:11.591469Z",
     "start_time": "2021-04-22T16:42:11.588759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('was','cat')\n",
    "probability_bigram(word1, word2, bigram_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:42:12.344442Z",
     "start_time": "2021-04-22T16:42:12.342212Z"
    }
   },
   "outputs": [],
   "source": [
    "def interpolation_probability(word1, word2, bigram_freq_dict, n_vocabulary, lambda_1 = 0.3): \n",
    "    return (1-lambda_1)*probability_bigram(word1, word2, bigram_freq_dict) +\\\n",
    "            lambda_1*prob_word(word2, unigrams, n_total_words, n_vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:47.034120Z",
     "start_time": "2021-04-22T16:03:47.030078Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09686619623303266"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('was','a')\n",
    "interpolation_probability(word1, word2, bigram_freq_dict, n_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:47.888220Z",
     "start_time": "2021-04-22T16:03:47.885084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00020633683341739648"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1, word2  = ('was','cat')\n",
    "interpolation_probability(word1, word2, bigram_freq_dict, n_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:50.700473Z",
     "start_time": "2021-04-22T16:03:50.697900Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_candidates(token, vocabulary, max_dist):\n",
    "    distance_token_to_words = {word:edit_distance(word,token.lower()) for word in vocabulary}\n",
    "    minimum_distance = min(distance_token_to_words.values())\n",
    "    if minimum_distance < max_dist:\n",
    "        return sorted([word for word, distance in distance_token_to_words.items() if distance == minimum_distance])\n",
    "    return [token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:51.751081Z",
     "start_time": "2021-04-22T16:03:51.687537Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house', 'huge', 'hus', 'use']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates('huse', vocabulary, max_dist=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:53.788186Z",
     "start_time": "2021-04-22T16:03:53.712985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['witch']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates('kitch', vocabulary, max_dist=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:55.177273Z",
     "start_time": "2021-04-22T16:03:55.175508Z"
    }
   },
   "outputs": [],
   "source": [
    "#%timeit get_candidates('hose', vocabulary, max_dist=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:55.428453Z",
     "start_time": "2021-04-22T16:03:55.424964Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def correct_with_bigrams(sentence, vocabulary):\n",
    "    for index,word in enumerate(sentence):\n",
    "        if ((word and word.lower()) not in vocabulary) and (not word[0].isupper()):\n",
    "            if index == 0: \n",
    "                previous_word = '.'\n",
    "            else:\n",
    "                previous_word = sentence[index-1].lower()\n",
    "            candidates = {candidate:interpolation_probability(previous_word, candidate, bigram_freq_dict,\n",
    "                                                              n_vocabulary, lambda_1=0.3) for candidate in get_candidates(word,vocabulary,max_dist=2)}\n",
    "            \n",
    "            \n",
    "            sentence[index] = max(candidates, key=candidates.get)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:03:56.433115Z",
     "start_time": "2021-04-22T16:03:56.367915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'big', 'house']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_with_bigrams([\"the\",\"big\",\"hose\"], vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:04:00.671690Z",
     "start_time": "2021-04-22T16:04:00.598626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rose': 6.87789444724655e-05,\n",
       " 'hole': 0.0019202404016783775,\n",
       " 'home': 0.0006602778669356688,\n",
       " 'homse': 0.0,\n",
       " 'hope': 0.00012380210005043788,\n",
       " 'horse': 1.37557888944931e-05,\n",
       " 'hoses': 1.37557888944931e-05,\n",
       " 'house': 0.00660255290938049,\n",
       " 'lose': 4.12673666834793e-05,\n",
       " 'nose': 4.12673666834793e-05,\n",
       " 'rose': 6.87789444724655e-05,\n",
       " 'those': 2.75115777889862e-05}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'hose'\n",
    "previous_word = 'the'\n",
    "candidates = {candidate:interpolation_probability(previous_word, candidate, bigram_freq_dict,n_vocabulary, lambda_1=0.3) for candidate in get_candidates(word,vocabulary,max_dist=2)}\n",
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-16T10:16:08.243Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_bigrams(test, vocabulary):\n",
    "    # Write your code here\n",
    "    count_total_words = 0\n",
    "    count_corrected_words = 0\n",
    "    mistakes = []\n",
    "    for m,sentence in enumerate(test):\n",
    "        s_true = sentence['corrected']\n",
    "        s_hat  = correct_with_bigrams(sentence['original'].copy(), vocabulary)\n",
    "        count_total_words  += len(s_true)\n",
    "        correct_predictions = sum(s_hat[n] == s_true[n] for n in range(len(s_true)))\n",
    "        count_corrected_words += correct_predictions\n",
    "        if correct_predictions != len(s_true):\n",
    "            mistakes.append(m)\n",
    "            \n",
    "    return count_corrected_words/count_total_words, mistakes\n",
    "\n",
    "acc, mistakes = accuracy_bigrams(test, vocabulary)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-16T10:16:09.840Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 14\n",
    "sentence = test[mistakes[i]]\n",
    "x = sentence[\"original\"]\n",
    "y_true = sentence[\"corrected\"]\n",
    "y_hat  = correct_with_bigrams(sentence['original'].copy(), vocabulary)\n",
    "\n",
    "print(f'mistake indices = {sentence[\"indexes\"]}')\n",
    "print(f'x      = {x}')\n",
    "print(f'y_hat  = {y_hat}')\n",
    "print(f'y_true = {y_true}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-16T10:16:10.829Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 4\n",
    "sentence = test[mistakes[i]]\n",
    "\n",
    "x      = sentence[\"original\"]\n",
    "y_true = sentence[\"corrected\"]\n",
    "y_hat  = correct_with_bigrams(sentence['original'].copy(), vocabulary)\n",
    "\n",
    "print(f'mistakes[i]={mistakes[i]}')\n",
    "print(f'mistake indices = {sentence[\"indexes\"]}')\n",
    "print(f'x      = {x}')\n",
    "print(f'y_hat  = {y_hat}')\n",
    "print(f'y_true = {y_true}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T09:44:05.852052Z",
     "start_time": "2021-04-16T09:44:05.849102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': ['on', 'sundays', 'I', 'go', 'to', 'church', '.'],\n",
       " 'corrected': ['on', 'sundays', 'I', 'go', 'to', 'church', '.'],\n",
       " 'indexes': []}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T09:44:06.740212Z",
     "start_time": "2021-04-16T09:44:06.637516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistakes[i]=33\n",
      "mistake indices = [15, 26]\n",
      "x      = ['The', 'murder', 'man', 'has', 'a', 'black', 'beard', 'The', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killd', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'Hill', 'the', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n",
      "y_hat  = ['The', 'murder', 'man', 'has', 'a', 'black', 'heard', 'The', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killed', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'Hill', 'the', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n",
      "y_true = ['The', 'murder', 'man', 'has', 'a', 'black', 'beard', 'The', 'next', 'day', 'one', 'of', 'the', 'policemen', 'were', 'killed', 'the', 'next', 'day', 'they', 'found', 'the', 'car', 'over', 'the', 'Hill', 'there', 'was', 'the', 'man', 'near', 'it', 'he', 'was', 'dead', '.']\n"
     ]
    }
   ],
   "source": [
    "i = 20\n",
    "sentence = test[mistakes[i]]\n",
    "x      = sentence[\"original\"]\n",
    "y_true = sentence[\"corrected\"]\n",
    "y_hat  = correct_with_bigrams(sentence['original'].copy(), vocabulary)\n",
    "\n",
    "\n",
    "print(f'mistakes[i]={mistakes[i]}')\n",
    "print(f'mistake indices = {sentence[\"indexes\"]}')\n",
    "print(f'x      = {x}')\n",
    "print(f'y_hat  = {y_hat}')\n",
    "print(f'y_true = {y_true}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
